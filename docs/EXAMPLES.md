# ML Container Creator - Examples

This guide provides step-by-step examples for common use cases.

## Table of Contents

- [Example 1: Deploy a scikit-learn Model](#example-1-deploy-a-scikit-learn-model)
- [Example 2: Deploy an XGBoost Model](#example-2-deploy-an-xgboost-model)
- [Example 3: Deploy a TensorFlow Model](#example-3-deploy-a-tensorflow-model)
- [Example 4: Deploy a Transformer Model (LLM)](#example-4-deploy-a-transformer-model-llm)
- [Example 5: Custom Configuration](#example-5-custom-configuration)

---

## Example 1: Deploy a scikit-learn Model

### Scenario
You have a trained scikit-learn model saved as `model.pkl` and want to deploy it to SageMaker with Flask serving.

### Step 1: Generate Project

```bash
yo ml-container-creator
```

**Prompts and Answers:**
```
ðŸ“‹ Project Configuration
? What is the Project Name? sklearn-iris-classifier
? Where will the output directory be? ./sklearn-iris-classifier-2024-12-02

ðŸ”§ Core Configuration
? Which ML framework are you using? sklearn
? In which format is your model serialized? pkl
? Which model server are you serving with? flask

ðŸ“¦ Module Selection
? Include sample Abalone classifier? No
? Include test suite? Yes
? Test type? local-model-cli, local-model-server, hosted-model-endpoint

ðŸ’ª Infrastructure & Performance
? Deployment target? sagemaker
? Instance type? cpu-optimized
? Target AWS region? us-east-1
```

### Step 2: Add Your Model

```bash
cd sklearn-iris-classifier-2024-12-02
cp /path/to/your/model.pkl code/model.pkl
```

### Step 3: Test Locally

```bash
# Build Docker image
docker build -t sklearn-iris-classifier .

# Run container locally
docker run -p 8080:8080 sklearn-iris-classifier

# Test in another terminal
curl -X POST http://localhost:8080/invocations \
  -H "Content-Type: application/json" \
  -d '{"instances": [[5.1, 3.5, 1.4, 0.2]]}'
```

### Step 4: Deploy to SageMaker

```bash
# Build and push to ECR
./deploy/build_and_push.sh

# Deploy to SageMaker (replace with your IAM role ARN)
./deploy/deploy.sh arn:aws:iam::123456789012:role/SageMakerExecutionRole
```

### Step 5: Test Endpoint

```bash
# Test the deployed endpoint
./test/test_endpoint.sh sklearn-iris-classifier
```

---

## Example 2: Deploy an XGBoost Model

### Scenario
You have an XGBoost model saved in JSON format for a regression task.

### Step 1: Generate Project

```bash
yo ml-container-creator
```

**Configuration:**
- Project Name: `xgboost-house-prices`
- Framework: `xgboost`
- Model Format: `json`
- Model Server: `fastapi`
- Include sample model: `No`
- Include tests: `Yes`
- Instance type: `cpu-optimized`

### Step 2: Prepare Model

```python
# Save your XGBoost model in JSON format
import xgboost as xgb

# Train your model
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Save as JSON
model.save_model('model.json')
```

```bash
# Copy to project
cp model.json xgboost-house-prices-*/code/model.json
```

### Step 3: Customize Inference (Optional)

Edit `code/model_handler.py` to customize preprocessing:

```python
def preprocess(data):
    """Custom preprocessing for house price features."""
    # Add your preprocessing logic
    return processed_data
```

### Step 4: Deploy

```bash
cd xgboost-house-prices-*
./deploy/build_and_push.sh
./deploy/deploy.sh arn:aws:iam::123456789012:role/SageMakerExecutionRole
```

---

## Example 3: Deploy a TensorFlow Model

### Scenario
You have a TensorFlow Keras model for image classification.

### Step 1: Generate Project

**Configuration:**
- Project Name: `tensorflow-image-classifier`
- Framework: `tensorflow`
- Model Format: `SavedModel`
- Model Server: `flask`
- Include sample model: `No`
- Include tests: `Yes`
- Instance type: `gpu-enabled`

### Step 2: Save Model in SavedModel Format

```python
import tensorflow as tf

# Train your model
model = tf.keras.Sequential([...])
model.compile(...)
model.fit(X_train, y_train)

# Save as SavedModel
model.save('saved_model')
```

```bash
# Copy to project
cp -r saved_model tensorflow-image-classifier-*/code/
```

### Step 3: Update Model Handler

Edit `code/model_handler.py` to handle image preprocessing:

```python
import numpy as np
from PIL import Image
import io

def preprocess(data):
    """Preprocess image data."""
    # Decode base64 image
    image = Image.open(io.BytesIO(data))
    # Resize and normalize
    image = image.resize((224, 224))
    image_array = np.array(image) / 255.0
    return np.expand_dims(image_array, axis=0)
```

### Step 4: Deploy to GPU Instance

```bash
cd tensorflow-image-classifier-*
./deploy/build_and_push.sh
./deploy/deploy.sh arn:aws:iam::123456789012:role/SageMakerExecutionRole
```

The deployment script will automatically select a GPU instance (ml.g4dn.xlarge) based on your configuration.

---

## Example 4: Deploy a Transformer Model (LLM)

### Scenario
You want to deploy a Llama 2 7B model using vLLM for efficient inference.

### Step 1: Generate Project

**Configuration:**
- Project Name: `llama2-7b-chat`
- Framework: `transformers`
- Model Server: `vllm`
- Include sample model: `No` (not applicable)
- Include tests: `Yes`
- Test types: `hosted-model-endpoint` (only option)
- Instance type: `gpu-enabled` (required)

### Step 2: Prepare Model Files

Option A: Download from Hugging Face Hub
```bash
cd llama2-7b-chat-*

# Install huggingface-cli
pip install huggingface-hub

# Download model (requires HF token for gated models)
huggingface-cli login
huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir ./model
```

Option B: Use existing local model
```bash
cp -r /path/to/llama2-model/* llama2-7b-chat-*/model/
```

### Step 3: Upload Model to S3

```bash
cd llama2-7b-chat-*

# Upload model to S3 (script will prompt for bucket name)
./deploy/upload_to_s3.sh
```

### Step 4: Update Dockerfile

Edit `Dockerfile` to specify model location:

```dockerfile
# Set model path (S3 or local)
ENV MODEL_NAME="meta-llama/Llama-2-7b-chat-hf"
# Or for S3: ENV MODEL_NAME="s3://my-bucket/models/llama2-7b"
```

### Step 5: Deploy

```bash
./deploy/build_and_push.sh
./deploy/deploy.sh arn:aws:iam::123456789012:role/SageMakerExecutionRole
```

**Note:** Transformer deployments require:
- GPU instance (ml.g4dn.xlarge or larger)
- Sufficient memory for model size
- S3 access permissions in IAM role

### Step 6: Test Inference

```bash
# Test the endpoint
aws sagemaker-runtime invoke-endpoint \
  --endpoint-name llama2-7b-chat \
  --content-type application/json \
  --body '{"inputs": "What is machine learning?", "parameters": {"max_new_tokens": 100}}' \
  output.json

cat output.json
```

---

## Example 5: Custom Configuration

### Scenario
You want to customize the generated project for specific requirements.

### Custom Requirements File

After generation, edit `requirements.txt`:

```txt
# Add custom dependencies
flask==2.3.0
scikit-learn==1.3.0
pandas==2.0.0
numpy==1.24.0

# Add your custom packages
my-custom-preprocessing==1.0.0
```

### Custom Model Handler

Edit `code/model_handler.py`:

```python
import logging
import numpy as np
from typing import Any, Dict

logger = logging.getLogger(__name__)

class ModelHandler:
    """Custom model handler with advanced preprocessing."""
    
    def __init__(self, model_path: str):
        self.model = self._load_model(model_path)
        self.preprocessor = self._load_preprocessor()
    
    def _load_model(self, path: str):
        """Load model with custom logic."""
        logger.info(f"Loading model from {path}")
        # Your custom loading logic
        return model
    
    def _load_preprocessor(self):
        """Load custom preprocessor."""
        # Your preprocessing pipeline
        return preprocessor
    
    def preprocess(self, data: Dict[str, Any]) -> np.ndarray:
        """Custom preprocessing logic."""
        # Feature engineering
        # Normalization
        # Encoding
        return processed_data
    
    def predict(self, data: np.ndarray) -> np.ndarray:
        """Run inference with custom post-processing."""
        predictions = self.model.predict(data)
        # Custom post-processing
        return predictions
```

### Custom Nginx Configuration

Edit `nginx.conf` for custom timeouts:

```nginx
http {
    # Increase timeout for slow models
    proxy_read_timeout 300s;
    proxy_connect_timeout 300s;
    
    # Increase buffer sizes for large payloads
    client_max_body_size 10M;
    
    upstream gunicorn {
        server unix:/tmp/gunicorn.sock;
    }
    
    server {
        listen 8080 deferred;
        client_max_body_size 10M;
        
        location ~ ^/(ping|invocations) {
            proxy_pass http://gunicorn;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header Host $http_host;
            proxy_redirect off;
        }
    }
}
```

### Custom Deployment Script

Edit `deploy/deploy.sh` to customize instance type:

```bash
#!/bin/bash

# Use larger instance for production
INSTANCE_TYPE="ml.m5.2xlarge"
INSTANCE_COUNT=2  # Multiple instances for HA

# Create endpoint configuration with auto-scaling
aws sagemaker create-endpoint-config \
  --endpoint-config-name ${PROJECT_NAME}-config \
  --production-variants \
    VariantName=AllTraffic,\
    ModelName=${PROJECT_NAME}-model,\
    InstanceType=${INSTANCE_TYPE},\
    InitialInstanceCount=${INSTANCE_COUNT}
```

---

## Common Patterns

### Pattern 1: Multi-Model Endpoint

Deploy multiple models to the same endpoint:

```python
# code/model_handler.py
class MultiModelHandler:
    def __init__(self):
        self.models = {
            'model_a': load_model('model_a.pkl'),
            'model_b': load_model('model_b.pkl')
        }
    
    def predict(self, data):
        model_name = data.get('model', 'model_a')
        return self.models[model_name].predict(data['input'])
```

### Pattern 2: Batch Prediction

Handle batch requests efficiently:

```python
def predict(self, data):
    """Handle batch predictions."""
    instances = data.get('instances', [])
    
    # Process in batches
    batch_size = 32
    predictions = []
    
    for i in range(0, len(instances), batch_size):
        batch = instances[i:i+batch_size]
        batch_pred = self.model.predict(batch)
        predictions.extend(batch_pred)
    
    return predictions
```

### Pattern 3: A/B Testing

Deploy multiple variants:

```bash
# Create endpoint config with multiple variants
aws sagemaker create-endpoint-config \
  --endpoint-config-name ab-test-config \
  --production-variants \
    VariantName=VariantA,ModelName=model-v1,InstanceType=ml.m5.xlarge,InitialInstanceCount=1,InitialVariantWeight=50 \
    VariantName=VariantB,ModelName=model-v2,InstanceType=ml.m5.xlarge,InitialInstanceCount=1,InitialVariantWeight=50
```

---

## Troubleshooting Examples

### Issue: Model File Not Found

**Error:**
```
FileNotFoundError: Model not found at /opt/ml/model/model.pkl
```

**Solution:**
```bash
# Verify model file is in correct location
ls -la code/
# Should show: model.pkl

# Rebuild container
docker build -t my-model .

# Verify model is in container
docker run my-model ls -la /opt/ml/model/
```

### Issue: Out of Memory

**Error:**
```
Container killed due to memory limit
```

**Solution:**
```bash
# Use larger instance type
# Edit deploy/deploy.sh
INSTANCE_TYPE="ml.m5.2xlarge"  # 32GB RAM instead of 16GB

# Or optimize model
# Use model quantization or pruning
```

### Issue: Slow Inference

**Problem:** Predictions take too long

**Solution:**
```python
# Load model once at startup, not per request
class ModelHandler:
    def __init__(self):
        self.model = load_model()  # Load once
    
    def predict(self, data):
        return self.model.predict(data)  # Reuse loaded model

# Use batch prediction
# Enable GPU acceleration
# Consider model optimization (ONNX, TensorRT)
```

---

## Next Steps

- Review [Project Documentation](https://github.com/awslabs/ml-container-creator)
- Check [Project Architecture](architecture.md) for detailed context
- Explore [Template Documentation](template-system.md)
- Read [AWS SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/)

## Contributing Examples

Have a useful example? Please contribute!

1. Fork the repository
2. Add your example to this file
3. Test the example end-to-end
4. Submit a pull request

Include:
- Clear scenario description
- Step-by-step instructions
- Expected output
- Common issues and solutions
